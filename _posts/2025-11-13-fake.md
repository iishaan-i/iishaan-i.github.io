---
layout: post
title: Base Models Know How to Reason, Thinking Models Learn When
date: 2025-11-13
paper_date: Oct 2025
authors: Constantin Venhoff, Iv√°n Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda
paper_url: https://arxiv.org/abs/2510.07364
tags: [LLMs, Intepretability]
summary: Analysis of SAE features to identify independent and interpretable reasoning mechanisms, create steering vectors to apply these mechanisms to base models, and recover up to 91% of the performance gap (hence the title)
---

## What they did
- Trained SAEs on thinking model activations
- Analyzed the activations to create a *complete* set of *independent* and *interpretable* reasoning mechanisms (they define these criteria clearly and show how they are achieved)
- Used the SAE-identified reasoning mechanisms to create steering vectors that activate these behaviors in *base models*
- Applied the reasoning behaviors to base models, but *only whenever the thinking model used them*

## Interesting Findings
-  Recovered 91% of the performance gap between the QwQ-32B (thinking) and Qwen2.5-32B (base) on MATH500. This is convincing, the original performance gap is 23%
- They note that applying the steering vectors at random times does not work as well, so *the timing the thinking model learns is important* (hence the title)

## What they did that feels really new
- When identifying reasining mechanisms, the unsupervised SAE-based discovery avoids the bias of manual annotation, unlike previous work
- Using thinking model activations as an oracle to decide *when* to apply steering vectors in base models, unlike previous work