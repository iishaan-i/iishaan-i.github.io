---
layout: post
title: Base Models Know How to Reason, Thinking Models Learn When
date: 2025-11-13
paper_date: Oct 2025
authors: Constantin Venhoff, Iv√°n Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda
paper_url: https://arxiv.org/abs/2510.07364
tags: [LLM, Interpretability]
summary: Clustering of LLM features using SAEs identifies independent and interpretable reasoning mechanisms, steering vectors apply these mechanisms to base models, up to 91% of the performance gap is recovered when steering in-sync with the reasoning model (hence the title)
---

## What they did
- Trained small latent dimension SAEs on sentence-level activations from thinking model (in hope that the latents would naturally form clusters representing fundamental reasoning mechanisms)
- Used an LLM to label the clusters by cognitive function based on the top activated sentences
- Evaluated clusters for completeness, consistency, and independence, to choose the hyperparameters that created the best reasoning mechanism "taxonomy"
- Used the identified reasoning mechanisms to create steering vectors that activate these behaviors in *base models*
- Applied the reasoning behaviors to base models, but *only whenever the thinking model used them*

## Interesting Findings
-  Recovered 91% of the performance gap between the QwQ-32B (thinking) and Qwen2.5-32B (base) on MATH500. This is convincing, the original performance gap is 23%
- They note that applying the steering vectors at random times performs notably worse, so *the timing the thinking model learns is important* (hence the title)

## What they did that feels really new
- When identifying reasoning mechanisms, the unsupervised SAE-based discovery + systematic evaluation of reasoning mechanism taxonomies avoids the bias of manual annotation, unlike previous work
- Using thinking model activations as an oracle to decide *when* to apply steering vectors in base models, unlike previous work